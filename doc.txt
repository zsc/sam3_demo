SAM 3 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºå›¾åƒå’Œè§†é¢‘ä¸­çš„å¯æç¤ºåˆ†å‰²ã€‚å®ƒå¯ä»¥ä½¿ç”¨æ–‡æœ¬æˆ–è§†è§‰æç¤ºï¼ˆå¦‚ç‚¹ã€æ¡†å’Œæ©ç ï¼‰æ¥æ£€æµ‹ã€åˆ†å‰²å’Œè·Ÿè¸ªå¯¹è±¡ã€‚ä¸å®ƒçš„å‰èº« SAM 2 ç›¸æ¯”ï¼ŒSAM 3 å¼•å…¥äº†æ ¹æ®ç®€çŸ­çš„æ–‡æœ¬çŸ­è¯­æˆ–ç¤ºä¾‹è¯¦å°½åœ°åˆ†å‰²æ‰€æœ‰å¼€æ”¾è¯æ±‡æ¦‚å¿µå®ä¾‹çš„èƒ½åŠ›ã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼ŒSAM 3 å¯ä»¥å¤„ç†æ›´å¤§èŒƒå›´çš„å¼€æ”¾è¯æ±‡æç¤ºã€‚åœ¨æˆ‘ä»¬çš„æ–° SA-CO åŸºå‡†æµ‹è¯• ä¸Šï¼Œå®ƒè¾¾åˆ°äº†äººç±»è¡¨ç°çš„ 75-80%ï¼Œè¯¥åŸºå‡†åŒ…å« 27 ä¸‡ä¸ªç‹¬ç‰¹æ¦‚å¿µï¼Œæ¯”ç°æœ‰åŸºå‡†å¤šå‡º 50 å¤šå€ã€‚

åŸºæœ¬ç”¨æ³•
import torch
#################################### For Image ####################################
from PIL import Image
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor
# Load the model
model = build_sam3_image_model()
processor = Sam3Processor(model)
# Load an image
image = Image.open("<YOUR_IMAGE_PATH.jpg>")
inference_state = processor.set_image(image)
# Prompt the model with text
output = processor.set_text_prompt(state=inference_state, prompt="<YOUR_TEXT_PROMPT>")

# Get the masks, bounding boxes, and scores
masks, boxes, scores = output["masks"], output["boxes"], output["scores"]

#################################### For Video ####################################

from sam3.model_builder import build_sam3_video_predictor

video_predictor = build_sam3_video_predictor()
video_path = "<YOUR_VIDEO_PATH>" # a JPEG folder or an MP4 video file
# Start a session
response = video_predictor.handle_request(
    request=dict(
        type="start_session",
        resource_path=video_path,
    )
)
response = video_predictor.handle_request(
    request=dict(
        type="add_prompt",
        session_id=response["session_id"],
        frame_index=0, # Arbitrary frame index
        text="<YOUR_TEXT_PROMPT>",
    )
)
output = response["outputs"]
å®˜æ–¹ä»£ç å·²åœ¨ sam3 ä»“åº“ ä¸­å…¬å¼€å‘å¸ƒã€‚

ä½¿ç”¨ ğŸ¤— Transformers
SAM3 - å›¾åƒçš„å¯æç¤ºæ¦‚å¿µåˆ†å‰² (PCS)
SAM3 å¯¹å›¾åƒæ‰§è¡Œå¯æç¤ºæ¦‚å¿µåˆ†å‰² (PCS)ï¼Œæ¥å—æ–‡æœ¬å’Œ/æˆ–å›¾åƒç¤ºä¾‹ä½œä¸ºæç¤ºï¼Œå¹¶è¿”å›å›¾åƒä¸­æ‰€æœ‰åŒ¹é…å¯¹è±¡å®ä¾‹çš„åˆ†å‰²æ©ç ã€‚

ä»…æ–‡æœ¬æç¤º
>>> from transformers import Sam3Processor, Sam3Model
>>> import torch
>>> from PIL import Image
>>> import requests

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> model = Sam3Model.from_pretrained("facebook/sam3").to(device)
>>> processor = Sam3Processor.from_pretrained("facebook/sam3")

>>> # Load image
>>> image_url = "http://images.cocodataset.org/val2017/000000077595.jpg"
>>> image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")

>>> # Segment using text prompt
>>> inputs = processor(images=image, text="ear", return_tensors="pt").to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )[0]

>>> print(f"Found {len(results['masks'])} objects")
>>> # Results contain:
>>> # - masks: Binary masks resized to original image size
>>> # - boxes: Bounding boxes in absolute pixel coordinates (xyxy format)
>>> # - scores: Confidence scores
æ‚¨å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ç®€å•çš„è¾…åŠ©å·¥å…·æ˜¾ç¤ºæ©ç ï¼š

import numpy as np
import matplotlib

def overlay_masks(image, masks):
    image = image.convert("RGBA")
    masks = 255 * masks.cpu().numpy().astype(np.uint8)
    
    n_masks = masks.shape[0]
    cmap = matplotlib.colormaps.get_cmap("rainbow").resampled(n_masks)
    colors = [
        tuple(int(c * 255) for c in cmap(i)[:3])
        for i in range(n_masks)
    ]

    for mask, color in zip(masks, colors):
        mask = Image.fromarray(mask)
        overlay = Image.new("RGBA", image.size, color + (0,))
        alpha = mask.point(lambda v: int(v * 0.5))
        overlay.putalpha(alpha)
        image = Image.alpha_composite(image, overlay)
    return image
ç„¶åæ‚¨å¯ä»¥ä¿å­˜ç”Ÿæˆçš„åˆæˆå›¾åƒæˆ–åœ¨ç¬”è®°æœ¬ä¸­æ˜¾ç¤ºå®ƒï¼š

>>> overlay_masks(image, results["masks"])
å•ä¸ªè¾¹ç•Œæ¡†æç¤º
ä½¿ç”¨è¾¹ç•Œæ¡†åˆ†å‰²å¯¹è±¡ï¼š

>>> # Box in xyxy format: [x1, y1, x2, y2] in pixel coordinates
>>> # Example: laptop region
>>> box_xyxy = [100, 150, 500, 450]
>>> input_boxes = [[box_xyxy]]  # [batch, num_boxes, 4]
>>> input_boxes_labels = [[1]]  # 1 = positive box

>>> inputs = processor(
...     images=image,
...     input_boxes=input_boxes,
...     input_boxes_labels=input_boxes_labels,
...     return_tensors="pt"
... ).to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )[0]
å¤šä¸ªè¾¹ç•Œæ¡†æç¤ºï¼ˆæ­£å‘å’Œè´Ÿå‘ï¼‰
ä½¿ç”¨å¸¦æœ‰æ­£å‘å’Œè´Ÿå‘æ ‡ç­¾çš„å¤šä¸ªè¾¹ç•Œæ¡†æ¥ç»†åŒ–æ¦‚å¿µï¼š

>>> # Load kitchen image
>>> kitchen_url = "http://images.cocodataset.org/val2017/000000136466.jpg"
>>> kitchen_image = Image.open(requests.get(kitchen_url, stream=True).raw).convert("RGB")

>>> # Define two positive boxes (e.g., dial and button on oven)
>>> # Boxes are in xyxy format [x1, y1, x2, y2] in pixel coordinates
>>> box1_xyxy = [59, 144, 76, 163]  # Dial box
>>> box2_xyxy = [87, 148, 104, 159]  # Button box
>>> input_boxes = [[box1_xyxy, box2_xyxy]]
>>> input_boxes_labels = [[1, 1]]  # Both positive

>>> inputs = processor(
...     images=kitchen_image,
...     input_boxes=input_boxes,
...     input_boxes_labels=input_boxes_labels,
...     return_tensors="pt"
... ).to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )[0]
>>> overlay_masks(kitchen_image, results["masks"])
ç»„åˆæç¤ºï¼ˆæ–‡æœ¬ + è´Ÿå‘è¾¹ç•Œæ¡†ï¼‰
ä½¿ç”¨æ–‡æœ¬æç¤ºä¸è´Ÿå‘è§†è§‰æç¤ºæ¥ç»†åŒ–æ¦‚å¿µï¼š

>>> # Segment "handle" but exclude the oven handle using a negative box
>>> text = "handle"
>>> # Negative box covering oven handle area (xyxy): [40, 183, 318, 204]
>>> oven_handle_box = [40, 183, 318, 204]
>>> input_boxes = [[oven_handle_box]]

>>> inputs = processor(
...     images=kitchen_image,
...     text=text,
...     input_boxes=input_boxes,
...     input_boxes_labels=[[0]],  # 0 = negative (exclude this region)
...     return_tensors="pt"
... ).to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )[0]
>>> # This will segment pot handles but exclude the oven handle
æ‰¹é‡æ¨ç†ä¸æ–‡æœ¬æç¤º
é€šè¿‡æ‰¹é‡å¤„ç†å…·æœ‰ä¸åŒæ–‡æœ¬æç¤ºçš„å¤šå¼ å›¾åƒï¼š

>>> cat_url = "http://images.cocodataset.org/val2017/000000077595.jpg"
>>> kitchen_url = "http://images.cocodataset.org/val2017/000000136466.jpg"
>>> images = [
...     Image.open(requests.get(cat_url, stream=True).raw).convert("RGB"),
...     Image.open(requests.get(kitchen_url, stream=True).raw).convert("RGB")
... ]

>>> text_prompts = ["ear", "dial"]

>>> inputs = processor(images=images, text=text_prompts, return_tensors="pt").to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results for both images
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )

>>> print(f"Image 1: {len(results[0]['masks'])} objects found")
>>> print(f"Image 2: {len(results[1]['masks'])} objects found")
æ··åˆæç¤ºæ‰¹é‡å¤„ç†
åœ¨åŒä¸€æ‰¹æ¬¡ä¸­å¯¹ä¸åŒå›¾åƒä½¿ç”¨ä¸åŒç±»å‹çš„æç¤ºï¼š

>>> # Image 1: text prompt "laptop"
>>> # Image 2: visual prompt (dial box)
>>> box2_xyxy = [59, 144, 76, 163]

>>> inputs = processor(
...     images=images,
...     text=["laptop", None],  # Only first image has text
...     input_boxes=[None, [box2_xyxy]],  # Only second image has box
...     input_boxes_labels=[None, [1]],  # Positive box for second image
...     return_tensors="pt"
... ).to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Post-process results for both images
>>> results = processor.post_process_instance_segmentation(
...     outputs,
...     threshold=0.5,
...     mask_threshold=0.5,
...     target_sizes=inputs.get("original_sizes").tolist()
... )
>>> # Both images processed in single forward pass
è¯­ä¹‰åˆ†å‰²è¾“å‡º
SAM3 è¿˜æä¾›ä¸å®ä¾‹æ©ç ä¸€èµ·çš„è¯­ä¹‰åˆ†å‰²ï¼š

>>> inputs = processor(images=image, text="ear", return_tensors="pt").to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # Instance segmentation masks
>>> instance_masks = torch.sigmoid(outputs.pred_masks)  # [batch, num_queries, H, W]

>>> # Semantic segmentation (single channel)
>>> semantic_seg = outputs.semantic_seg  # [batch, 1, H, W]

>>> print(f"Instance masks: {instance_masks.shape}")
>>> print(f"Semantic segmentation: {semantic_seg.shape}")
SAM3 è§†é¢‘ - è§†é¢‘çš„å¯æç¤ºæ¦‚å¿µåˆ†å‰² (PCS)
SAM3 è§†é¢‘å¯¹è§†é¢‘æ‰§è¡Œå¯æç¤ºæ¦‚å¿µåˆ†å‰² (PCS)ï¼Œæ¥å—æ–‡æœ¬ä½œä¸ºæç¤ºï¼Œå¹¶åœ¨è§†é¢‘å¸§ä¸­æ£€æµ‹å’Œè·Ÿè¸ªæ‰€æœ‰åŒ¹é…çš„å¯¹è±¡å®ä¾‹ã€‚

é¢„åŠ è½½è§†é¢‘æ¨ç†
ä½¿ç”¨æ–‡æœ¬æç¤ºå¤„ç†æ‰€æœ‰å¸§éƒ½å·²å¯ç”¨çš„è§†é¢‘ï¼š

>>> from transformers import Sam3VideoModel, Sam3VideoProcessor
>>> from accelerate import Accelerator
>>> import torch

>>> device = Accelerator().device
>>> model = Sam3VideoModel.from_pretrained("facebook/sam3").to(device, dtype=torch.bfloat16)
>>> processor = Sam3VideoProcessor.from_pretrained("facebook/sam3")

>>> # Load video frames
>>> from transformers.video_utils import load_video
>>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"
>>> video_frames, _ = load_video(video_url)

>>> # Initialize video inference session
>>> inference_session = processor.init_video_session(
...     video=video_frames,
...     inference_device=device,
...     processing_device="cpu",
...     video_storage_device="cpu",
...     dtype=torch.bfloat16,
... )

>>> # Add text prompt to detect and track objects
>>> text = "person"
>>> inference_session = processor.add_text_prompt(
...     inference_session=inference_session,
...     text=text,
... )

>>> # Process all frames in the video
>>> outputs_per_frame = {}
>>> for model_outputs in model.propagate_in_video_iterator(
...     inference_session=inference_session, max_frame_num_to_track=50
... ):
...     processed_outputs = processor.postprocess_outputs(inference_session, model_outputs)
...     outputs_per_frame[model_outputs.frame_idx] = processed_outputs

>>> print(f"Processed {len(outputs_per_frame)} frames")
Processed 51 frames

>>> # Access results for a specific frame
>>> frame_0_outputs = outputs_per_frame[0]
>>> print(f"Detected {len(frame_0_outputs['object_ids'])} objects")
>>> print(f"Object IDs: {frame_0_outputs['object_ids'].tolist()}")
>>> print(f"Scores: {frame_0_outputs['scores'].tolist()}")
>>> print(f"Boxes shape (XYXY format, absolute coordinates): {frame_0_outputs['boxes'].shape}")
>>> print(f"Masks shape: {frame_0_outputs['masks'].shape}")
æµå¼è§†é¢‘æ¨ç†
å¯¹äºå®æ—¶åº”ç”¨ï¼ŒTransformers å®ç°çš„ SAM3 è§†é¢‘æ”¯æŒåœ¨è§†é¢‘å¸§åˆ°è¾¾æ—¶è¿›è¡Œå¤„ç†ï¼š

>>> # Initialize session for streaming
>>> streaming_inference_session = processor.init_video_session(
...     inference_device=device,
...     processing_device="cpu",
...     video_storage_device="cpu",
...     dtype=torch.bfloat16,
... )

>>> # Add text prompt
>>> text = "person"
>>> streaming_inference_session = processor.add_text_prompt(
...     inference_session=streaming_inference_session,
...     text=text,
... )

>>> # Process frames one by one (streaming mode)
>>> streaming_outputs_per_frame = {}
>>> for frame_idx, frame in enumerate(video_frames[:50]):  # Process first 50 frames
...     # First, process the frame using the processor
...     inputs = processor(images=frame, device=device, return_tensors="pt")
...
...     # Process frame using streaming inference - pass the processed pixel_values
...     model_outputs = model(
...         inference_session=streaming_inference_session,
...         frame=inputs.pixel_values[0],  # Provide processed frame - this enables streaming mode
...         reverse=False,
...     )
...
...     # Post-process outputs with original_sizes for proper resolution handling
...     processed_outputs = processor.postprocess_outputs(
...         streaming_inference_session,
...         model_outputs,
...         original_sizes=inputs.original_sizes,  # Required for streaming inference
...     )
...     streaming_outputs_per_frame[frame_idx] = processed_outputs
...
...     if (frame_idx + 1) % 10 == 0:
...         print(f"Processed {frame_idx + 1} frames...")

>>> print(f"âœ“ Streaming inference complete! Processed {len(streaming_outputs_per_frame)} frames")
âœ“ Streaming inference complete! Processed 50 frames

>>> # Access results
>>> frame_0_outputs = streaming_outputs_per_frame[0]
>>> print(f"Detected {len(frame_0_outputs['object_ids'])} objects in first frame")
>>> print(f"Boxes are in XYXY format (absolute pixel coordinates): {frame_0_outputs['boxes'].shape}")
>>> print(f"Masks are at original video resolution: {frame_0_outputs['masks'].shape}")
âš ï¸ **å…³äºæµå¼æ¨ç†è´¨é‡çš„è¯´æ˜**ï¼šæµå¼æ¨ç†ç¦ç”¨äº†ç§»é™¤æœªåŒ¹é…å’Œé‡å¤å¯¹è±¡çš„çƒ­å¯åŠ¨å¯å‘å¼æ–¹æ³•ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•éœ€è¦è®¿é—®æœªæ¥çš„å¸§æ‰èƒ½åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚è¿™å¯èƒ½å¯¼è‡´æ›´å¤šçš„è¯¯æŠ¥æ£€æµ‹å’Œé‡å¤å¯¹è±¡è½¨è¿¹ï¼Œç›¸æ¯”äºé¢„åŠ è½½è§†é¢‘æ¨ç†ã€‚ä¸ºäº†è·å¾—æœ€ä½³ç»“æœï¼Œåœ¨æ‰€æœ‰å¸§éƒ½å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œè¯·ä½¿ç”¨é¢„åŠ è½½è§†é¢‘æ¨ç†ã€‚
SAM3 Tracker - å›¾åƒçš„å¯æç¤ºè§†è§‰åˆ†å‰² (PVS)
Sam3Tracker å¯¹å›¾åƒæ‰§è¡Œå¯æç¤ºè§†è§‰åˆ†å‰² (PVS)ï¼Œæ¥å—äº¤äº’å¼è§†è§‰æç¤ºï¼ˆç‚¹ã€æ¡†ã€æ©ç ï¼‰ä»¥æ ¹æ®æ¯ä¸ªæç¤ºåˆ†å‰²ç‰¹å®šå¯¹è±¡å®ä¾‹ã€‚å®ƒæ˜¯ SAM2 çš„æ›´æ–°ç‰ˆæœ¬ï¼Œä¿æŒç›¸åŒçš„ API åŒæ—¶æä¾›äº†æ”¹è¿›çš„æ€§èƒ½ï¼Œä½¿å…¶æˆä¸º SAM2 å·¥ä½œæµç¨‹çš„ç›´æ¥æ›¿ä»£å“ã€‚

ä½¿ç”¨ç®¡é“è‡ªåŠ¨ç”Ÿæˆæ©ç 
>>> from transformers import pipeline

>>> generator = pipeline("mask-generation", model="facebook/sam3", device=0)
>>> image_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg"
>>> outputs = generator(image_url, points_per_batch=64)

>>> len(outputs["masks"])  # Number of masks generated
åŸºæœ¬å›¾åƒåˆ†å‰²
å•ç‚¹ç‚¹å‡»
>>> from transformers import Sam3TrackerProcessor, Sam3TrackerModel
>>> from accelerate import Accelerator
>>> import torch
>>> from PIL import Image
>>> import requests

>>> device = Accelerator().device

>>> model = Sam3TrackerModel.from_pretrained("facebook/sam3").to(device)
>>> processor = Sam3TrackerProcessor.from_pretrained("facebook/sam3")

>>> image_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg"
>>> raw_image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")

>>> input_points = [[[[500, 375]]]]  # Single point click, 4 dimensions (image_dim, object_dim, point_per_object_dim, coordinates)
>>> input_labels = [[[1]]]  # 1 for positive click, 0 for negative click, 3 dimensions (image_dim, object_dim, point_label)

>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors="pt").to(model.device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"])[0]

>>> # The model outputs multiple mask predictions ranked by quality score
>>> print(f"Generated {masks.shape[1]} masks with shape {masks.shape}")
å¤šç‚¹ç»†åŒ–
>>> # Add both positive and negative points to refine the mask
>>> input_points = [[[[500, 375], [1125, 625]]]]  # Multiple points for refinement
>>> input_labels = [[[1, 1]]]  # Both positive clicks

>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors="pt").to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"])[0]
è¾¹ç•Œæ¡†è¾“å…¥
>>> # Define bounding box as [x_min, y_min, x_max, y_max]
>>> input_boxes = [[[75, 275, 1725, 850]]]

>>> inputs = processor(images=raw_image, input_boxes=input_boxes, return_tensors="pt").to(device)

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"])[0]
å¤šå¯¹è±¡åˆ†å‰²
>>> # Define points for two different objects
>>> input_points = [[[[500, 375]], [[650, 750]]]]  # Points for two objects in same image
>>> input_labels = [[[1], [1]]]  # Positive clicks for both objects

>>> inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors="pt").to(model.device)

>>> with torch.no_grad():
...     outputs = model(**inputs, multimask_output=False)

>>> # Each object gets its own mask
>>> masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"])[0]
>>> print(f"Generated masks for {masks.shape[0]} objects")
Generated masks for 2 objects
æ‰¹é‡æ¨ç†
>>> # Load multiple images
>>> image_urls = [
...     "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg",
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png"
... ]
>>> raw_images = [Image.open(requests.get(url, stream=True).raw).convert("RGB") for url in image_urls]

>>> # Single point per image
>>> input_points = [[[[500, 375]]], [[[770, 200]]]]  # One point for each image
>>> input_labels = [[[1]], [[1]]]  # Positive clicks for both images

>>> inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors="pt").to(model.device)

>>> with torch.no_grad():
...     outputs = model(**inputs, multimask_output=False)

>>> # Post-process masks for each image
>>> all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"])
>>> print(f"Processed {len(all_masks)} images, each with {all_masks[0].shape[0]} objects")
SAM3 Tracker è§†é¢‘ - è§†é¢‘çš„å¯æç¤ºè§†è§‰åˆ†å‰² (PVS)
Sam3TrackerVideo å¯¹è§†é¢‘æ‰§è¡Œå¯æç¤ºè§†è§‰åˆ†å‰² (PVS)ï¼Œæ¥å—äº¤äº’å¼è§†è§‰æç¤ºï¼ˆç‚¹ã€æ¡†ã€æ©ç ï¼‰ä»¥è·¨è§†é¢‘å¸§è·Ÿè¸ªæ¯ä¸ªæç¤ºçš„ç‰¹å®šå¯¹è±¡å®ä¾‹ã€‚å®ƒæ˜¯ SAM2 è§†é¢‘çš„æ›´æ–°ç‰ˆæœ¬ï¼Œä¿æŒç›¸åŒçš„ API åŒæ—¶æä¾›äº†æ”¹è¿›çš„æ€§èƒ½ï¼Œä½¿å…¶æˆä¸º SAM2 è§†é¢‘å·¥ä½œæµç¨‹çš„ç›´æ¥æ›¿ä»£å“ã€‚

åŸºæœ¬è§†é¢‘è·Ÿè¸ª
>>> from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor
>>> from accelerate import Accelerator
>>> import torch

>>> device = Accelerator().device
>>> model = Sam3TrackerVideoModel.from_pretrained("facebook/sam3").to(device, dtype=torch.bfloat16)
>>> processor = Sam3TrackerVideoProcessor.from_pretrained("facebook/sam3")

>>> # Load video frames
>>> from transformers.video_utils import load_video
>>> video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"
>>> video_frames, _ = load_video(video_url)

>>> # Initialize video inference session
>>> inference_session = processor.init_video_session(
...     video=video_frames,
...     inference_device=device,
...     dtype=torch.bfloat16,
... )

>>> # Add click on first frame to select object
>>> ann_frame_idx = 0
>>> ann_obj_id = 1
>>> points = [[[[210, 350]]]]
>>> labels = [[[1]]]

>>> processor.add_inputs_to_inference_session(
...     inference_session=inference_session,
...     frame_idx=ann_frame_idx,
...     obj_ids=ann_obj_id,
...     input_points=points,
...     input_labels=labels,
... )

>>> # Segment the object on the first frame (optional, you can also propagate the masks through the video directly)
>>> outputs = model(
...     inference_session=inference_session,
...     frame_idx=ann_frame_idx,
... )
>>> video_res_masks = processor.post_process_masks(
...     [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False
... )[0]
>>> print(f"Segmentation shape: {video_res_masks.shape}")
Segmentation shape: torch.Size([1, 1, 480, 854])

>>> # Propagate through the entire video
>>> video_segments = {}
>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):
...     video_res_masks = processor.post_process_masks(
...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False
...     )[0]
...     video_segments[sam3_tracker_video_output.frame_idx] = video_res_masks

>>> print(f"Tracked object through {len(video_segments)} frames")
Tracked object through 180 frames
å¤šå¯¹è±¡è§†é¢‘è·Ÿè¸ª
åŒæ—¶è·¨è§†é¢‘å¸§è·Ÿè¸ªå¤šä¸ªå¯¹è±¡ï¼š

>>> # Reset for new tracking session
>>> inference_session.reset_inference_session()

>>> # Add multiple objects on the first frame
>>> ann_frame_idx = 0
>>> obj_ids = [2, 3]
>>> input_points = [[[[200, 300]], [[400, 150]]]]  # Points for two objects (batched)
>>> input_labels = [[[1], [1]]]

>>> processor.add_inputs_to_inference_session(
...     inference_session=inference_session,
...     frame_idx=ann_frame_idx,
...     obj_ids=obj_ids,
...     input_points=input_points,
...     input_labels=input_labels,
... )

>>> # Get masks for both objects on first frame (optional, you can also propagate the masks through the video directly)
>>> outputs = model(
...     inference_session=inference_session,
...     frame_idx=ann_frame_idx,
... )

>>> # Propagate both objects through video
>>> video_segments = {}
>>> for sam3_tracker_video_output in model.propagate_in_video_iterator(inference_session):
...     video_res_masks = processor.post_process_masks(
...         [sam3_tracker_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=False
...     )[0]
...     video_segments[sam3_tracker_video_output.frame_idx] = {
...         obj_id: video_res_masks[i]
...         for i, obj_id in enumerate(inference_session.obj_ids)
...     }

>>> print(f"Tracked {len(inference_session.obj_ids)} objects through {len(video_segments)} frames")
Tracked 2 objects through 180 frames
æµå¼è§†é¢‘æ¨ç†
å¯¹äºå®æ—¶åº”ç”¨ï¼ŒSam3TrackerVideo æ”¯æŒåœ¨è§†é¢‘å¸§åˆ°è¾¾æ—¶è¿›è¡Œå¤„ç†ï¼š

>>> # Initialize session for streaming
>>> inference_session = processor.init_video_session(
...     inference_device=device,
...     dtype=torch.bfloat16,
... )

>>> # Process frames one by one
>>> for frame_idx, frame in enumerate(video_frames[:10]):  # Process first 10 frames
...     inputs = processor(images=frame, device=device, return_tensors="pt")
...
...     if frame_idx == 0:
...         # Add point input on first frame
...         processor.add_inputs_to_inference_session(
...             inference_session=inference_session,
...             frame_idx=0,
...             obj_ids=1,
...             input_points=[[[[210, 350], [250, 220]]]],
...             input_labels=[[[1, 1]]],
...             original_size=inputs.original_sizes[0], # need to be provided when using streaming video inference
...         )
...
...     # Process current frame
...     sam3_tracker_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[0])
...
...     video_res_masks = processor.post_process_masks(
...         [sam3_tracker_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=False
...     )[0]
...     print(f"Frame {frame_idx}: mask shape {video_res_masks.shape}")
